{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook to fine tune XGB Model iteratively  \n",
    "\n",
    "**This notebook predicts TotalLOS given patient demographics, primay Diagnosis, Diagnosis category and comorbidities**  \n",
    "  \n",
    "The documentation from running this notebook can be found here  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import *\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('data/DiagVsLOSRangev5.csv', index_col=[0])\n",
    "data = rawdata.copy()\n",
    "CatgCols = [ 'Gender', 'Ethinicity', 'Religion', 'MaritalStatus',\n",
    "             'DiagnosisCategory', 'PrimaryDiag', 'DiagGroup']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Preprocess the data for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eliminate some noise based on data exploration\n",
    "# data[CatgCols[4]].value_counts()\n",
    "# Make PrimaryDiag = Other if only one patient\n",
    "v = data['PrimaryDiag'].value_counts() == 1 \n",
    "data.loc[data['PrimaryDiag'].isin(v.index[v]), 'PrimaryDiag'] = 'Other'\n",
    "# Make Religion = Other if less than 5 patients\n",
    "v = data['Religion'].value_counts() < 5  \n",
    "data.loc[data['Religion'].isin(v.index[v]), 'Religion'] = 'Other'\n",
    "# Make DiagnosisCategory = Other if less than 10 patients\n",
    "v = data['DiagnosisCategory'].value_counts() < 10  \n",
    "data.loc[data['DiagnosisCategory'].isin(v.index[v]), 'DiagnosisCategory'] = 'Other'\n",
    "# Make DiagGroup = Other if less than 10 patients\n",
    "v = data['DiagGroup'].value_counts() < 10  \n",
    "data.loc[data['DiagGroup'].isin(v.index[v]), 'DiagGroup'] = 'Other'\n",
    "# Make Ethinicity = Other if less than 10 patients\n",
    "v = data['Ethinicity'].value_counts() < 10  \n",
    "data.loc[data['Ethinicity'].isin(v.index[v]), 'Ethinicity'] = 'Other'\n",
    "\n",
    "### TODO  \"Create a pipeline function to deal with unseen categories\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Make this a code cell if using LabelEncoder\n",
    "# Label Encoder\n",
    "#encode categorical columns\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "data = rawdata\n",
    "data[CatgCols].values.flatten()\n",
    "encoder.fit(data[CatgCols].values.flatten().astype(str))\n",
    "data[CatgCols] = encoder.transform(data[CatgCols].values.flatten().astype(str)).reshape(data[CatgCols].shape)\n",
    "\n",
    "data_ohe = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoder: Mark this cell as Raw if label Encoder is in use\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# creating one hot encoder object \n",
    "data_ohe = data\n",
    "for col in CatgCols:\n",
    "    col_ohe = pd.get_dummies(data[col], prefix=col)\n",
    "    data_ohe = pd.concat((data_ohe, col_ohe), axis=1).drop(col, axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6287, 520)\n",
      "(6287, 331)\n"
     ]
    }
   ],
   "source": [
    "print(data_ohe.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose target and unused columns in the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ignore the first three columns (LosRange, EntitySys, TotalLOS and )\n",
    "target='TotalLos'\n",
    "NonFeatureCols =['LosRange','EntitySys','DiagDesc','TotalLos']   # always include predictor\n",
    "featureList = [x for x in data_ohe.columns if x not in NonFeatureCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Split Train and Test Data sets for validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices and dataframes for train/test\n",
    "X = data_ohe[featureList]\n",
    "y = data_ohe[target]\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.70)\n",
    "X_valid, X_test,y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n",
    "\n",
    "\n",
    "my_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "train_X = my_imputer.fit_transform(X_train)\n",
    "test_X = my_imputer.transform(X_test)\n",
    "valid_X = my_imputer.transform(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions for use in tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import XGBFunctions as myf\n",
    "#imports the following functions\n",
    "#   printPredictions(alg, X, y, featureList):\n",
    "#   printResults(alg, X, y, featureList):\n",
    "#   modelfit(alg, xtrain, ytrain, target, featureList,useTrainCV=True, \n",
    "#            cv_folds=5, early_stopping_rounds=50):\n",
    "#   printModelStats(model, dtrain, dtest, featureList, target):\n",
    "#   save_tree(xgb_model, filename, rankdir='UT'):\n",
    "#   show_tree(model):\n",
    "#   xgbfit(alg, xtrain, ytrain, featureList):\n",
    "#   xgbRfit(alg, xtrain, ytrain, featureList):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "defaultmodel = xgb.XGBRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators':  [50,100],\n",
    "    'max_depth':     [10,20,30]\n",
    "}\n",
    "xgb_gs = GridSearchCV(defaultmodel, param_grid, cv=5, scoring=['neg_mean_absolute_error'],\n",
    "                      refit='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters found by grid search are: {xgb_gs.best_params_}')\n",
    "print(f'Best parameters found by grid search are: {xgb_gs._estimator_.names_steps}')\n",
    "\n",
    "'''\n",
    "Best parameters found by grid search are: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 50}\n",
    "CPU times: user 25.2 s, sys: 628 ms, total: 25.8 s\n",
    "Wall time: 2h 27min 24s\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "defaultmodel = xgb.XGBRegressor(n_estimators=50, learning_rate=0.01,max_depth=10, verbose=1)\n",
    "\n",
    "param_grid = {\n",
    "    'reg_alpha': [0.001, 0.01, 0.1, 0.2],\n",
    "    'reg_lambda':  [0.1, 0.5,1.0]\n",
    "}\n",
    "xgb_gs = GridSearchCV(defaultmodel, param_grid, cv=5, scoring=['neg_mean_absolute_error'],\n",
    "                      refit='neg_mean_absolute_error', n_jobs=-1)\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters found by grid search are: {xgb_gs.best_params_}')\n",
    "\n",
    "'''\n",
    "Best parameters found by grid search are: {'reg_alpha': 0.2, 'reg_lambda': 1.0}\n",
    "CPU times: user 24.9 s, sys: 420 ms, total: 25.4 s\n",
    "Wall time: 30min 22s\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'tuple'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bbd2f88ea411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m'subsample'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m }\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mxgbm_bo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbm_re_bo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_gbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mxgbm_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'It takes %s minutes'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, pbounds, random_state, verbose, bounds_transformer)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Data structure containing the function to be optimized, the bounds of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# its domain, and a record of the evaluations we have done so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTargetSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target_func, pbounds, random_state)\u001b[0m\n\u001b[1;32m     47\u001b[0m         self._bounds = np.array(\n\u001b[1;32m     48\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbounds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "# XGBM\n",
    "# Objective function\n",
    "def xgbm_re_bo(max_depth, reg_alpha, reg_lambda, learning_rate, n_estimators, subsample):\n",
    "    params_gbm = {}\n",
    "    params_gbm['max_depth'] = round(max_depth)\n",
    "#    params_gbm['reg_alpha'] = reg_alpha\n",
    "#    params_gbm['reg_lambda'] = reg_lambda\n",
    "    params_gbm['learning_rate'] = learning_rate\n",
    "    params_gbm['n_estimators'] = round(n_estimators)\n",
    "    params_gbm['subsample'] = subsample\n",
    "    scores = cross_val_score(XGBRegressor(random_state=77, **params_gbm),\n",
    "                             X_train, y_train, scoring='neg_mean_absolute_error', cv=5).mean()\n",
    "    score = scores.mean()\n",
    "    return score\n",
    "# Run Bayesian Optimization\n",
    "start = time.time()\n",
    "params_gbm ={\n",
    "    'max_depth': (3, 10),\n",
    "    'reg_alpha': (0.8, 0.9),\n",
    "    'reg_lambda': (0.001, .01, 0.1),\n",
    "    'learning_rate': (0.01, 0.1, 0.5, 1.0),\n",
    "    'n_estimators': (80, 150, 200),\n",
    "    'subsample': (0.8, 1)\n",
    "}\n",
    "xgbm_bo = BayesianOptimization(xgbm_re_bo, params_gbm, random_state=77)\n",
    "xgbm_bo.minimize(n_iter=4)\n",
    "print('It takes %s minutes' % ((time.time() - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TargetSpace' from 'bayes_opt' (/opt/conda/lib/python3.7/site-packages/bayes_opt/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-93e5b1d5f93f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbayes_opt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTargetSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mTargetSpace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbR1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_gbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TargetSpace' from 'bayes_opt' (/opt/conda/lib/python3.7/site-packages/bayes_opt/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "best_model=xgb_gs.best_estimator_\n",
    "#pd.DataFrame({'Feature':SelectFromModel(best_model, max_features=20).feature_names_,\n",
    "#              'Threshold':SelectFromModel(best_model, max_features=20).threshold_}, index=None)\n",
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not edit above code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change parameters and run to record results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xgbR1 = XGBRegressor(\n",
    "    n_estimators=60,           # num_boosting_rounds passed to fit function\n",
    "    use_label_encoder=False,\n",
    "    enable_categorical = False,  # only for gpu_hist tree methods\n",
    "\n",
    "    # general parameters\n",
    "    max_depth= 10,              # default=6 (3-10) lower  underfits\n",
    "    colsample_bytree= 1,       # cols to sample for each tree\n",
    "    colsample_bynode= 1,       # cols to sample for each split node\n",
    "    max_bin=5,                 ## minuimum 2  This parameter has no effect on anything. Manual Grid search.\n",
    "    learning_rate = 0.1,\n",
    "    verbosity= 2,              # 0:silent 1: Info 2: Warn 3: debug\n",
    "\n",
    "    booster= 'gbtree',\n",
    "    #booster parameters\n",
    "    min_child_weight= 5,       ## default=1        higher underfits Train and test consistently.\n",
    "    gamma=0,                   ## depends on loss function minimum loss needed to split set to regularize\n",
    "    max_delta_step= 0,         # 0 is disabled, upper limit for wt neeeded to split the tree\n",
    "    subsample= 1,              # sample observations for each tree 1 means all\n",
    "\n",
    "    colsample_bylevel= 1,      # column sample at each level  Finer tuning to fix issues with data\n",
    "    reg_lambda=1.0,          # L2 regularization evenly reduce of wts\n",
    "    reg_alpha=0.2,             # L1 regularization eliminate weights randomly \n",
    "    # learning parameters\n",
    "    objective= 'reg:squarederror',  # loss function objective (reduce squared error)  \n",
    "    tree_method= 'auto',       # based on column histograms rather than reading observations every time.\n",
    "\n",
    "    base_score= 0.5,           ## initial prediction score for all instances (global bias)\n",
    "    random_state= 0,\n",
    "    validate_parameters= 1,\n",
    "    n_jobs=20\n",
    ")\n",
    "start_time = time.time()\n",
    "score = cross_val_score(xgbR1, X_train, y_train, scoring='neg_mean_absolute_error', cv=5, verbose=2).mean()\n",
    "#xgbR1.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10,\n",
    "#          callbacks = [xgb.callback.EarlyStopping(rounds=10, save_best=True)], verbose=True)\n",
    "#print(score.mean())\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Model Fit Time %s\\n\\n' % time.strftime(\"%Hh:%Mm:%Ss\", time.gmtime(elapsed)))\n",
    "xgbR1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Performance\n",
    "los = y_train.values\n",
    "pred = best_model.predict(X_train)\n",
    "diffs = abs(los-pred).round(0).astype(int)\n",
    "bins =  [-1,   7,      14,           30,          60,        90,         120,        np.inf]\n",
    "names = ['a:+/-7','b:+/-14',   'c:+/-30',   'd:+/-60',   'e:+/-90', 'f:+/-120',   'g:+180']\n",
    "                     \n",
    "pd.cut(abs(diffs), bins, labels=names).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Performance in how close predictions are to actual LOS\n",
    "## Training Data Performance\n",
    "los = y_train.values\n",
    "pred = xgbR1.predict(X_train)\n",
    "diffs = abs(los-pred).round(0).astype(int)\n",
    "bins =  [-1,   7,      14,           30,          60,        90,         120,        np.inf]\n",
    "names = ['a:+/-7','b:+/-14',   'c:+/-30',   'd:+/-60',   'e:+/-90', 'f:+/-120',   'g:+180']\n",
    "                     \n",
    "pd.cut(abs(diffs), bins, labels=names).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Performance in how close predictions are to actual LOS\n",
    "## Test Data Performance\n",
    "los = y_test.values\n",
    "pred = xgbR1.predict(X_test)\n",
    "diffs = abs(los-pred).round(0).astype(int)\n",
    "bins =  [-1,   7,      14,           30,          60,        90,         120,        np.inf]\n",
    "names = ['a:+/-7','b:+/-14',   'c:+/-30',   'd:+/-60',   'e:+/-90', 'f:+/-120',   'g:+180']\n",
    "                     \n",
    "pd.cut(abs(diffs), bins, labels=names).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data Performance\n",
    "los = y_valid.values\n",
    "pred = xgbR1.predict(X_valid)\n",
    "diffs = abs(los-pred).round(0).astype(int)\n",
    "bins =  [-1,   7,      14,           30,          60,        90,         120,        np.inf]\n",
    "names = ['a:+/-7','b:+/-14',   'c:+/-30',   'd:+/-60',   'e:+/-90', 'f:+/-120',   'g:+180']\n",
    "                     \n",
    "pd.cut(abs(diffs), bins, labels=names).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myf.show_tree(xgbR1, tree_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Pred':pred,'Los':los, 'Diff':pred-los}, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(sklearn.feature_selection.SelectFromModel))\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "SelectFromModel?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(best_model, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgbR1, max_num_features=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
