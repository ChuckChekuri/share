{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook to fine tune XGBM Model iteratively  \n",
    "\n",
    "**This notebook predicts Los Range given patient demographics, primay Diagnosis, Diagnosis category and comorbidities**  \n",
    "  \n",
    "\n",
    "### Step 1: Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_csv('data/NVtoDTDRange8.csv', index_col=[0])\n",
    "\n",
    "data = rawdata.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Preprocess the data for the model "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0:<7': 0, '1:7-14': 1, '2:15-21': 2, '3:22-30': 3, '4:31-60': 4, '5:61-120': 5, '6:121-180': 6, '7:180+': 7}\n"
     ]
    }
   ],
   "source": [
    "#Drop text column DiagDesc \n",
    "data.drop(columns='DiagDesc', inplace=True)\n",
    "\n",
    "#encode categorical columns\n",
    "CatgCols = ['Gender', 'Ethinicity', 'Religion', 'MaritalStatus',\n",
    "       'DiagnosisCategory', 'PrimaryDiag', 'DiagGroup']\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "data[CatgCols].values.flatten()\n",
    "le.fit(data[CatgCols].values.flatten().astype(str))\n",
    "data[CatgCols] = le.transform(data[CatgCols].values.flatten().astype(str)).reshape(data[CatgCols].shape)\n",
    "\n",
    "# encode predictor variable\n",
    "lep = preprocessing.LabelEncoder()\n",
    "lep.fit(data.LosRange)\n",
    "lep_dict = dict(zip(lep.classes_, lep.transform(lep.classes_)))\n",
    "print(lep_dict)\n",
    "data.LosRange = lep.transform(data.LosRange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose target and unused columns in the dataset**  \n",
    "Filter out columns/rows if chosing to build model for a specific group/category of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='LosRange'\n",
    "## Ignore the following columns\n",
    "NonFeatureCols =['LosRange','EntitySys','TotalLos','DiagDesc', 'DTDsinceVD',\n",
    "                 'AdmitMonth','AgeAtAdmit']   # always include predictor\n",
    "featureList = [x for x in data.columns if x not in NonFeatureCols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Split Train and Test Data sets for validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deal with missing values in train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices and dataframes for train/test\n",
    "X = data[featureList]\n",
    "y = data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "train_X = my_imputer.fit_transform(X_train)\n",
    "test_X = my_imputer.transform(X_test)\n",
    "#dtrain = my_imputer.fit_transform(dtrain)\n",
    "#dtest = my_imputer.transform(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search from lgbm parameters\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('grid search from lgbm parameters')\n",
    "\n",
    "# other scikit-learn modules\n",
    "defaultmodel = lgb.LGBMClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40, 60, 80, 100]\n",
    "}\n",
    "\n",
    "lgbm_cv = GridSearchCV(defaultmodel, param_grid, cv=3, n_jobs=-1)\n",
    "lgbm_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters found by grid search are: {lgbm_cv.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "defaultmodel = lgb.LGBMClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.2],\n",
    "    'n_estimators':  [60],\n",
    "    'max_depth':      [10],\n",
    "    'boosting_type': ['gbdt'],\n",
    "    'min_child_samples' : [30],\n",
    "    'min_split_gain' : [0.1],\n",
    "    'num_leaves': [50],\n",
    "    'min_child_weight': [ 0.01],\n",
    "    'subsample_freqency' : [40, 1,5,10,20, 3]\n",
    "}\n",
    "lgbm_cv = GridSearchCV(defaultmodel, param_grid, cv=4, scoring=['accuracy','precision_macro','recall_macro'],\n",
    "                       refit='accuracy', n_jobs=-1)\n",
    "lgbm_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters found by grid search are: {lgbm_cv.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Cell start here.  \n",
    "Modify the parameters for each run and document the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change parameters and run to record results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=60,           # num_boosting_rounds passed to fit function\n",
    "    learning_rate = 0.2,\n",
    "    # general parameters\n",
    "    max_depth= 10,              # default=6 (3-10) lower  underfits\n",
    "    boosting_type = 'gbdt',\n",
    "    colsample_bytree = 1.0,       # cols to sample for each tree\n",
    "    importance_type = 'gain',\n",
    "    min_child_samples = 30,\n",
    "    min_split_gain = 0.1,\n",
    "    n_jobs = -1,\n",
    "    subsample_freq = 40,\n",
    "    verbosity= 0,              # 0:silent 1: Info 2: Warn 3: debug\n",
    "    num_leaves = 50,           # maximum number of leaves \n",
    "    silent= 'warn',\n",
    "    #booster parameters\n",
    "    min_child_weight= 0.01,   ## default=1        higher underfits Train and test consistently.\n",
    "    max_delta_step= 0,       # 0 is disabled, upper limit for wt neeeded to split the tree\n",
    "    subsample= 1,             # sample observations for each tree 1 means all\n",
    "    subsample_for_bin = 200000, # sample observations for node\n",
    "    reg_lambda=1,              # L2 regularization evenly reduce of wts\n",
    "    reg_alpha=0,               # L1 regularization eliminate weights randomly \n",
    "    # learning parameters\n",
    "    objective= 'multiclass',  # multi:softmax for classes  \n",
    "    seed= 77                  # reproducible for parameter tuning. \n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "# train\n",
    "# train\n",
    "lgbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        callbacks=[lgb.early_stopping(6)])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print('Model Fit Time %s\\n\\n' % time.strftime(\"%Hh:%Mm:%Ss\", time.gmtime(elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# predict\n",
    "y_hat = lgbm.predict(X_train, num_iteration=lgbm.best_iteration_)\n",
    "# eval\n",
    "print(\"Training Results\")\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_train, y_hat) * 100.0))\n",
    "print('Precision: %0.5f' % precision_score(y_train, y_hat, average='macro'))\n",
    "print('Recall: %0.5f'    % recall_score(y_train, y_hat, average='macro'))\n",
    "\n",
    "# feature importances\n",
    "print(\"Training Confusion Matrix\")\n",
    "cfm = metrics.confusion_matrix(y_train, y_hat)\n",
    "print(cfm)\n",
    "\n",
    "print(\"Test Results\")\n",
    "y_pred = lgbm.predict(X_test, num_iteration=lgbm.best_iteration_)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test, y_pred) * 100.0))\n",
    "print('Precision: %0.5f' % precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: %0.5f'    % recall_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(\"Test Confusion Matrix\")\n",
    "\n",
    "cfm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(cfm)\n",
    "\n",
    "sorted_idx = lgbm.feature_importances_.argsort()\n",
    "plt.barh(np.array(featureList)[sorted_idx][-20:], lgbm.feature_importances_[sorted_idx][-20:],\n",
    "         height=0.85, align='center')\n",
    "plt.xlabel(\"LGBM Feature Importance\")\n",
    "\n",
    "lgbm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Performance in how close predictions are to actual LOS\n",
    "## Model Performance in how close predictions are to actual LOS\n",
    "XX=data.groupby('LosRange', as_index=False)['TotalLos'].mean()\n",
    "los_class_mean_map = dict(zip(XX.LosRange, XX.TotalLos))\n",
    "\n",
    "los = y_test.values\n",
    "pred = lgbm.predict(X_test[featureList])\n",
    "pred_los = pd.Series(pred).map(los_class_mean_map).values\n",
    "los_deviance = pd.Series(y_test).map(los_class_mean_map).values\n",
    "diffs1 = abs(los-pred_los).round(0).astype(int)\n",
    "diffs2 = abs(los_deviance-pred_los).round(0).astype(int)\n",
    "\n",
    "bins =  [-1,   7,      14,           30,          60,        90,         120,        np.inf]\n",
    "names = ['a:+/-7','b:+/-14',   'c:+/-30',   'd:+/-60',   'e:+/-90', 'f:+/-120',   'g:+180']\n",
    "\n",
    "#print(pd.cut(abs(diffs1), bins, labels=names).value_counts())\n",
    "print(pd.cut(abs(diffs2), bins, labels=names).value_counts())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
